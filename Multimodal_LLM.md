# 1. Transformer升级之路：17、多模态编码位置的简单思考 https://kexue.fm/archives/10040
![image](https://github.com/qianxinchun/awesomeLLMTopics/assets/7309139/fbfe50a9-054a-47d8-af67-408e2958247a)

# 2. “闭门造车”之多模态模型方案浅谈 https://kexue.fm/archives/9984
![image](https://github.com/qianxinchun/awesomeLLMTopics/assets/7309139/ae424930-37fd-4199-a937-76dfe8379167)
![image](https://github.com/qianxinchun/awesomeLLMTopics/assets/7309139/b6294df9-66e0-41f5-8fb8-8726d3f3008a)
![image](https://github.com/qianxinchun/awesomeLLMTopics/assets/7309139/c9875780-52bb-4428-9f89-0fd43866281d)

# 3. 在用llava架构训vlm时，llm基模选择base模型好还是chat模型好呢？ https://www.zhihu.com/question/650838721/answer/3448817012
原问题：看很多模型都是用base，但像mobilevlm用的chat模型效果指标也挺好。而且llava1.6的34b基模也是在Yi34b上finetune过的。所以有人做过实验测试vlm用base或chat的差别吗？



